{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "from glob import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import MeCab\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_txt(path): # 引数にはPDFファイルパスを指定\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    laparams.detect_vertical = True # Trueにすることで綺麗にテキストを抽出できる\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "    fstr = ''\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "        str = retstr.getvalue()\n",
    "        fstr += str\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return fstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-88e2c65f450e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_pdf_to_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# テキストの形態素解析\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWakati\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# 形態素解析結果を半角スペースで結合されたテキストに変換\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlinetext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens2linetext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-6da7fbf72f80>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# 分かち書きを行うメソッド\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stopped_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-6da7fbf72f80>\u001b[0m in \u001b[0;36mget_words\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# 形態素解析結果から目的となる品詞に一致した単語リストを取得する関数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mdfw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dfw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-6da7fbf72f80>\u001b[0m in \u001b[0;36mget_dfw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0msurface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0msurfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "path = \"documents/JPA-1999069893.pdf\"\n",
    "\n",
    "text = \"機発電装置とを組み合わせたハイブリッド発電システムにおいて、発電電力並びに負荷変動に対する電力系統の安定性を保つことにある。\"\n",
    "text = convert_pdf_to_txt(path).replace('\\n', '')\n",
    "# テキストの形態素解析\n",
    "tokens = Wakati(text).tokenize()\n",
    "# 形態素解析結果を半角スペースで結合されたテキストに変換\n",
    "linetext = tokens2linetext(tokens)\n",
    "fontpath = \"/System/Library/Fonts/ヒラギノ丸ゴ ProN W4.ttc\"\n",
    "# WordCloudの外形画像の準備\n",
    "# mask_path = \"datum_mask.png\"\n",
    "# datum_mask = np.array(Image.open(mask_path))\n",
    "# WordCloudを用いて単語頻度図を作成\n",
    "wc = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    colormap=\"winter\",\n",
    "    font_path=fontpath,\n",
    "    collocations=False,\n",
    "    height=674,\n",
    "    width=1024\n",
    ")\n",
    "wc.generate(linetext)\n",
    "# 出力ファイル名設定と単語頻度図の出力\n",
    "filename = os.path.basename(path)\n",
    "filename = os.path.splitext(filename)[0]\n",
    "outname = \"wcfig_\" + filename + \".png\"\n",
    "wc.to_file(outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "romaji = re.compile(\"[a-zA-Z]+\")\n",
    "text = romaji.sub(\"\", doc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "mecab = MeCab.Tagger (\"-Owakati\")\n",
    "text = convert_pdf_to_txt(path).replace('\\n', '')\n",
    "text = mecab.parse (text)\n",
    "pattern = r\"[0-9A-Za-zぁ-んァ-ヴ一-龠]\"\n",
    "stopwords = re.sub(pattern, '',text)\n",
    "# print(stopwords)\n",
    "\n",
    "def wakati_by_mecab(text):\n",
    "    tagger = MeCab.Tagger('')\n",
    "    tagger.parse('') \n",
    "    node = tagger.parseToNode(text)\n",
    "    word_list = []\n",
    "    while node:\n",
    "        pos = node.feature.split(\",\")[0]\n",
    "        if pos in [\"名詞\"]:   # 対象とする品詞\n",
    "            word = node.surface\n",
    "            word_list.append(word)\n",
    "        node = node.next\n",
    "    return \" \".join(word_list)\n",
    "\n",
    "def lower_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def normalize_number(text):\n",
    "    # 連続した数字を0で置換\n",
    "    replaced_text = re.sub(r'\\d+', '', text)\n",
    "    return replaced_text\n",
    "\n",
    "def download_stopwords(path):\n",
    "    url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "    if os.path.exists(path):\n",
    "        print('File already exists.')\n",
    "    else:\n",
    "        print('Downloading...')\n",
    "        # Download the file from `url` and save it locally under `file_name`:\n",
    "        urllib.request.urlretrieve(url, path)\n",
    "\n",
    "def create_stopwords(path):\n",
    "    stop_words = []\n",
    "    for w in open(path, \"r\"):\n",
    "        w = w.replace('\\n','')\n",
    "        if len(w) > 0:\n",
    "          stop_words.append(w)\n",
    "    return stop_words    \n",
    "\n",
    "file_path = \"/Users/yusuke-n/code/work/read_pdf/stopword.txt\"\n",
    "# text = wakati_by_mecab(text)\n",
    "# text = lower_text(text)\n",
    "# text = normalize_number(text)\n",
    "download_stopwords(file_path)\n",
    "stop_words = create_stopwords(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "comments = [\"あなたは春が好きね\", text]\n",
    "sample_dict = {\"comment\": comments} \n",
    "samples = pd.DataFrame.from_dict(sample_dict)\n",
    "samples[\"target\"] = samples[\"comment\"].apply(wakati_by_mecab).apply(lower_text).apply(normalize_number)\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words=stop_words,token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\n",
    "feature_vectors = count_vectorizer.fit_transform(samples['target'])\n",
    "vocabulary = count_vectorizer.get_feature_names() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(\n",
    "                      font_path=\"/system/Library/Fonts/ヒラギノ角ゴシック W4.ttc\",\n",
    "                      colormap='copper_r',\n",
    "                      width=800,height=600).generate(samples['target'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x12b272490>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcloud.to_file(\"./wordcloud_sample.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
